{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import praw\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from praw.models import MoreComments\n",
    "from gensim import models\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_comments = pd.read_csv(\"startup_comments.csv\", index_col=0)\n",
    "post_data = pd.read_csv(\"startup_posts.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_comments['comment_body']=post_comments['comment_body'].str.replace(\"ndosan\", 'dosan')\n",
    "post_comments['comment_body']=post_comments['comment_body'].str.replace(\"dal mi\", 'dalmi')\n",
    "post_comments['comment_body']=post_comments['comment_body'].str.replace(\"dm\", 'dalmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_link_id</th>\n",
       "      <th>Post Title</th>\n",
       "      <th>Post ID</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gepr1xu</td>\n",
       "      <td>t3_k7batc</td>\n",
       "      <td>##finale week\\n\\nwelcome to the final episode ...</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>Start-Up [Episode 16] FINALE</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>##finale week\\n\\nwelcome to the final episode ...</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geu9gq5</td>\n",
       "      <td>t3_k7batc</td>\n",
       "      <td>i'm genuinely confused on why the love triangl...</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>Start-Up [Episode 16] FINALE</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>i'm genuinely confused on why the love triangl...</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geuhy27</td>\n",
       "      <td>t3_k7batc</td>\n",
       "      <td>final thoughts: i don't think hjp and kim seon...</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>Start-Up [Episode 16] FINALE</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>final thoughts: i don't think hjp and kim seon...</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geu82v5</td>\n",
       "      <td>t3_k7batc</td>\n",
       "      <td>from this in ep 1:\\n\\n\"don't call me when you ...</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>Start-Up [Episode 16] FINALE</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>from this in ep 1:\\n\\n\"don't call me when you ...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geu82ew</td>\n",
       "      <td>t3_k7batc</td>\n",
       "      <td>hosting the weekly discussions for start-up ha...</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>Start-Up [Episode 16] FINALE</td>\n",
       "      <td>k7batc</td>\n",
       "      <td>hosting the weekly discussions for start-up ha...</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29345</th>\n",
       "      <td>gbuup6a</td>\n",
       "      <td>t1_gbutsyb</td>\n",
       "      <td>yeah me too i really wish for plot progress an...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>[Start Up] Cinematography Appreciation : Rooft...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>yeah me too i really wish for plot progress an...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29347</th>\n",
       "      <td>gbtjfcf</td>\n",
       "      <td>t1_gbt7hv8</td>\n",
       "      <td>yes, it took about 2 episodes to grow on me. i...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>[Start Up] Cinematography Appreciation : Rooft...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>yes, it took about 2 episodes to grow on me. i...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29348</th>\n",
       "      <td>gbuuwjm</td>\n",
       "      <td>t1_gbuup6a</td>\n",
       "      <td>if i‚Äôm not mistaken, there‚Äôs a sdm timeskip ha...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>[Start Up] Cinematography Appreciation : Rooft...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>if i‚Äôm not mistaken, there‚Äôs a sdm timeskip ha...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29349</th>\n",
       "      <td>gbwejmn</td>\n",
       "      <td>t1_gbvrgwd</td>\n",
       "      <td>yaaaaas we‚Äôre getting more kim seon ho!  he de...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>[Start Up] Cinematography Appreciation : Rooft...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>yaaaaas we‚Äôre getting more kim seon ho! üí™üèº he ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29350</th>\n",
       "      <td>gbvxbm5</td>\n",
       "      <td>t1_gbtjfcf</td>\n",
       "      <td>i see. then, i‚Äôll put that on my list. thanks ...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>[Start Up] Cinematography Appreciation : Rooft...</td>\n",
       "      <td>jr0ngl</td>\n",
       "      <td>i see. then, i‚Äôll put that on my list. thanks ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29071 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id comment_parent_id  \\\n",
       "0        gepr1xu         t3_k7batc   \n",
       "1        geu9gq5         t3_k7batc   \n",
       "2        geuhy27         t3_k7batc   \n",
       "3        geu82v5         t3_k7batc   \n",
       "4        geu82ew         t3_k7batc   \n",
       "...          ...               ...   \n",
       "29345    gbuup6a        t1_gbutsyb   \n",
       "29347    gbtjfcf        t1_gbt7hv8   \n",
       "29348    gbuuwjm        t1_gbuup6a   \n",
       "29349    gbwejmn        t1_gbvrgwd   \n",
       "29350    gbvxbm5        t1_gbtjfcf   \n",
       "\n",
       "                                            comment_body comment_link_id  \\\n",
       "0      ##finale week\\n\\nwelcome to the final episode ...          k7batc   \n",
       "1      i'm genuinely confused on why the love triangl...          k7batc   \n",
       "2      final thoughts: i don't think hjp and kim seon...          k7batc   \n",
       "3      from this in ep 1:\\n\\n\"don't call me when you ...          k7batc   \n",
       "4      hosting the weekly discussions for start-up ha...          k7batc   \n",
       "...                                                  ...             ...   \n",
       "29345  yeah me too i really wish for plot progress an...          jr0ngl   \n",
       "29347  yes, it took about 2 episodes to grow on me. i...          jr0ngl   \n",
       "29348  if i‚Äôm not mistaken, there‚Äôs a sdm timeskip ha...          jr0ngl   \n",
       "29349  yaaaaas we‚Äôre getting more kim seon ho!  he de...          jr0ngl   \n",
       "29350  i see. then, i‚Äôll put that on my list. thanks ...          jr0ngl   \n",
       "\n",
       "                                              Post Title Post ID  \\\n",
       "0                           Start-Up [Episode 16] FINALE  k7batc   \n",
       "1                           Start-Up [Episode 16] FINALE  k7batc   \n",
       "2                           Start-Up [Episode 16] FINALE  k7batc   \n",
       "3                           Start-Up [Episode 16] FINALE  k7batc   \n",
       "4                           Start-Up [Episode 16] FINALE  k7batc   \n",
       "...                                                  ...     ...   \n",
       "29345  [Start Up] Cinematography Appreciation : Rooft...  jr0ngl   \n",
       "29347  [Start Up] Cinematography Appreciation : Rooft...  jr0ngl   \n",
       "29348  [Start Up] Cinematography Appreciation : Rooft...  jr0ngl   \n",
       "29349  [Start Up] Cinematography Appreciation : Rooft...  jr0ngl   \n",
       "29350  [Start Up] Cinematography Appreciation : Rooft...  jr0ngl   \n",
       "\n",
       "                                           clean_comment  word_count  \n",
       "0      ##finale week\\n\\nwelcome to the final episode ...         648  \n",
       "1      i'm genuinely confused on why the love triangl...         382  \n",
       "2      final thoughts: i don't think hjp and kim seon...         236  \n",
       "3      from this in ep 1:\\n\\n\"don't call me when you ...         142  \n",
       "4      hosting the weekly discussions for start-up ha...         264  \n",
       "...                                                  ...         ...  \n",
       "29345  yeah me too i really wish for plot progress an...          33  \n",
       "29347  yes, it took about 2 episodes to grow on me. i...          36  \n",
       "29348  if i‚Äôm not mistaken, there‚Äôs a sdm timeskip ha...          27  \n",
       "29349  yaaaaas we‚Äôre getting more kim seon ho! üí™üèº he ...          12  \n",
       "29350  i see. then, i‚Äôll put that on my list. thanks ...          13  \n",
       "\n",
       "[29071 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', str(text))\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www.\\.\\S+')\n",
    "    return url_pattern.sub(r'', str(text))\n",
    "\n",
    "def removeStopwordsAndPunctuations(raw_text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    new_words = ['yes','true','exactly','lol','lmao','haha','hahaha', 'ikr','yup','yeah','right','omg','thank','agree','really','episode','think','also','get','would','even','u']\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    stopwords = stopwords.union(set(new_words))\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    words = tokenizer.tokenize(str(raw_text))\n",
    "    \n",
    "    wordsFiltered = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords:\n",
    "            wordsFiltered.append(word)\n",
    "    \n",
    "    lem = WordNetLemmatizer()\n",
    "    wordsLemmatized = []\n",
    "    for word in wordsFiltered:\n",
    "        wordsLemmatized.append(lem.lemmatize(word))\n",
    "            \n",
    "    text = ''\n",
    "    for w in wordsLemmatized:\n",
    "        text = text+' '+w.lower()\n",
    "    return str(text)\n",
    "\n",
    "post_comments['clean_comment'] = post_comments.apply(lambda row:remove_emoji(row['comment_body']), axis=1)\n",
    "post_comments['clean_comment'] = post_comments.apply(lambda row:remove_urls(row['comment_body']), axis=1)\n",
    "post_comments['clean_comment'] = post_comments.apply(lambda row:removeStopwordsAndPunctuations(row['comment_body']), axis=1)\n",
    "post_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(''.join(post_comments['clean_comment']).split()).value_counts()[:50]\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = post_comments['clean_comment'].tolist()\n",
    "wordcloud = WordCloud(background_color = 'white', max_words=100, max_font_size=50, random_state=42).generate(str(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordcloud)\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "fig.savefig('word1.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
